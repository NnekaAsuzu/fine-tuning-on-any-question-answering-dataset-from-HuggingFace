# -*- coding: utf-8 -*-
"""Finetuning on Hugging Face dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A41KZw58pfi0YzZzSEaxV3m3hjgDA6lB

**Project**: Perform fine-tuning on any question-answering dataset from HuggingFace and save the model. Use the saved model to build a Gradio interface. The interface should display a context window as the first input, a text field for the user's question as the second input, and the model's response as the output. This interface will allow users to input a context and a question to receive the model's answer.

#####Install the Transformers, Datasets, and Evaluate libraries to run this notebook.
"""

!pip install datasets evaluate transformers[sentencepiece]
!pip install accelerate
!apt install git-lfs
!pip install datasets
!pip install transformers datasets torch gradio

"""##setup git, adapt your email and name in the following cell"""

!git config --global user.email "nnasuzu@gmail.com"
!git config --global user.name "NnekaAsuzu"

"""##log in to the Hugging Face Hub"""

from huggingface_hub import notebook_login

notebook_login()

#Import  libraries
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments,DataCollatorWithPadding
import torch
from transformers import default_data_collator
from torch.utils.data import DataLoader
import gradio as gr

# Download Load the dataset Stanford Question Answering Dataset (SQuAD)

from datasets import load_dataset

dataset = load_dataset("Squad")

dataset #viewdatasets

dataset['train'][0] #to look at the data

# Load the tokenizer and model(in other to tokenize any sentences, we have to define the tockenizer first and load)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-uncased")
# Load a pretrained model
pretrained_model = AutoModelForQuestionAnswering.from_pretrained("bert-base-uncased")

#load checkpoint
checkpoint ='bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(checkpoint) #load tokenizer from the checkpoint

dataset['train']['context'][0] #to view the context

# Tokenize the dataset
def tokenize_function(example):
    return tokenizer(example["question"], example["context"], truncation=True)
    #map is used to apply tokenizer function in the whole dataset
#batch is used to apply function in batches and iterate over the dataset much faster

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# compare the orginal to the tockenized dataset ( more inputs are seen)
dataset

#To check the tokenized data
tokenized_datasets

"""Training with a Custom Trainer Method: In this method, the custom trainer class is used to manage the training process."""

# Define a custom training step
def compute_loss(model, inputs):
    outputs = model(**inputs)
    start_logits = outputs.start_logits
    end_logits = outputs.end_logits
    start_positions = inputs.get("start_positions")
    end_positions = inputs.get("end_positions")
    loss = None
    if start_positions is not None and end_positions is not None:
        loss_fct = torch.nn.CrossEntropyLoss()
        start_loss = loss_fct(start_logits, start_positions)
        end_loss = loss_fct(end_logits, end_positions)
        loss = (start_loss + end_loss) / 2
    return loss if loss is not None else torch.tensor(0.0, device=model.device, requires_grad=True)

#Define the trainer with the overridden compute_loss method
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs):
        return compute_loss(model, inputs)

# Define training arguments with modified logging steps
training_args = TrainingArguments(
    output_dir="./checkpoints",  # Directory to save checkpoints
    save_strategy="steps",        # Save checkpoints every few steps
    save_steps=500,               # Save a checkpoint every 500 steps
    logging_dir="./logs",         # Directory to save logs
    logging_steps=2000,           # Log metrics every 2000 steps
    overwrite_output_dir=True,    # Overwrite the output directory if it exists
    # Add other training arguments as needed
)

# Create an instance of the custom trainer with the modified training arguments
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=DataCollatorWithPadding(tokenizer),
)

# Train the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained("fine_tuned_squad_model")
tokenizer.save_pretrained("fine_tuned_squad_model")

# Load the saved model and tokenizer
model_path = "fine_tuned_squad_model"
model = AutoModelForQuestionAnswering.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

#Generate sample responses from the model
# Define a function to get the model's response/predict the answer
def answer_question(context, question):
    inputs = tokenizer(question, context, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]

    outputs = model(**inputs)
    answer_start = torch.argmax(outputs.start_logits)
    answer_end = torch.argmax(outputs.end_logits) + 1
    answer = tokenizer.decode(input_ids[answer_start:answer_end])

    return answer

# Context
context = """
AI is revolutionizing industries globally, impacting job markets.
"""

# Questions
question = [
    "How is AI revolutionizing industries globally?"
]

# Answer each question
for i, question in enumerate(question, 1):
    answer = answer_question(context, question)
    print(f"Question {i}: {question}\nAnswer: {answer}\n")

# Create a Gradio interface
context_input = gr.Textbox(lines=10, label="Context")
question_input = gr.Textbox(lines=2, label="Question")
output_text = gr.Textbox(label="Answer")

# Launch the interface
iface = gr.Interface(fn=answer_question, inputs=[context_input, question_input], outputs=output_text)
iface.launch()