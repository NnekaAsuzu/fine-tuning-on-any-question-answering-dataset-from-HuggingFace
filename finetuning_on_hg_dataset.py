# -*- coding: utf-8 -*-
"""Finetuning_on_HG_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ANFfwKS5-I5msp7QRMjHc6TTaBhg13fC

**Project**: Perform fine-tuning on any question-answering dataset from HuggingFace and save the model. Use the saved model to build a Gradio interface. The interface should display a context window as the first input, a text field for the user's question as the second input, and the model's response as the output. This interface will allow users to input a context and a question to receive the model's answer.

#####Install the Transformers, Datasets, and Evaluate libraries to run this notebook.
"""

!pip install datasets evaluate transformers[sentencepiece]
!pip install accelerate
!apt install git-lfs
!pip install datasets
!pip install transformers datasets torch gradio

"""##setup git, adapt your email and name in the following cell"""

!git config --global user.email "nnasuzu@gmail.com"
!git config --global user.name "NnekaAsuzu"

"""##log in to the Hugging Face Hub"""

from huggingface_hub import notebook_login

notebook_login()

#Import  libraries
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments,DataCollatorWithPadding
import torch
from transformers import default_data_collator
from torch.utils.data import DataLoader
import gradio as gr

# Download Load the dataset Stanford Question Answering Dataset (SQuAD)

from datasets import load_dataset

dataset = load_dataset("squad")

dataset #viewdatasets

#To see the first element of the training set
print("Context: ", dataset["train"][0]["context"])
print("Question: ", dataset["train"][0]["question"])
print("Answer: ",dataset["train"][0]["answers"])

dataset['train'][0] #to look at the data

#QAModel used:bert-large-uncased-whole-word-masking-finetuned-squad
# Define your model and tokenizer
model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Load a pretrained model
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

#load checkpoint
checkpoint ='bert-large-uncased-whole-word-masking-finetuned-squad'
tokenizer = AutoTokenizer.from_pretrained(checkpoint) #load tokenizer from the checkpoint

dataset['train']['context'][0] #to view the context

# Tokenize the dataset
def tokenize_function(example):
    return tokenizer(example["question"], example["context"], truncation=True)
    #map is used to apply tokenizer function in the whole dataset
#batch is used to apply function in batches and iterate over the dataset much faster

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# compare the orginal to the tockenized dataset ( more inputs are seen)
dataset

#To check the tokenized data
tokenized_datasets

"""Training with a Custom Trainer Method: In this method, the custom trainer class is used to manage the training process."""

# Define a custom training step
def compute_loss(model, inputs):
    outputs = model(**inputs)
    start_logits = outputs.start_logits
    end_logits = outputs.end_logits
    start_positions = inputs.get("start_positions")
    end_positions = inputs.get("end_positions")
    loss = None
    if start_positions is not None and end_positions is not None:
        loss_fct = torch.nn.CrossEntropyLoss()
        start_loss = loss_fct(start_logits, start_positions)
        end_loss = loss_fct(end_logits, end_positions)
        loss = (start_loss + end_loss) / 2
    return loss if loss is not None else torch.tensor(0.0, device=model.device, requires_grad=True)

#Define the trainer with the overridden compute_loss method
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs):
        return compute_loss(model, inputs)

# Define training arguments with fewer epochs and a smaller batch size
training_args = TrainingArguments(
    "output_dir",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,  # Decreased batch size
    per_device_eval_batch_size=2,   # Decreased batch size
    num_train_epochs=1,             # Reduced number of epochs
    weight_decay=0.01,
)

# Sample a smaller subset of the training dataset
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))

# Create an instance of the custom trainer with the updated training arguments
#Using the sample of smaller subset(500)
trainer = CustomTrainer(
    model=pretrained_model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=tokenized_datasets["validation"],
    data_collator=DataCollatorWithPadding(tokenizer),
)
trainer.train()

# Save the fine-tuned model
model.save_pretrained("fine_tuned_squad_model")

# Load the saved model
model = AutoModelForQuestionAnswering.from_pretrained("fine_tuned_squad_model")

# Define a function to get the model's response
def get_response(context, question):
    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
    input_ids = inputs["input_ids"].tolist()[0]

    outputs = model(**inputs)
    start_logits = outputs.start_logits
    end_logits = outputs.end_logits

    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
    answer = tokenizer.decode(input_ids[torch.argmax(start_logits) : torch.argmax(end_logits) + 1])

    return answer

# Example usages
examples = [
    ("Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems.", "What is artificial intelligence?"),
    ("Machine learning is a subset of artificial intelligence that focuses on the development of computer programs that can access data and use it to learn for themselves.", "What is machine learning?"),
    ("Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.", "What is natural language processing?"),
    ("Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data.", "What is deep learning?")
]

for context, question in examples:
    response = get_response(context, question)
    print("Context:", context)
    print("Question:", question)
    print("Model's Response:", response)
    print()

# Build the Gradio interface
def qa_app(context, question):
    answer = get_response(context, question)
    return answer

iface = gr.Interface(fn=qa_app, inputs=["text", "text"], outputs="text")
iface.launch()